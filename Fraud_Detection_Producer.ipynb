{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903568f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime as dt\n",
    "import random\n",
    "import csv\n",
    "from kafka3 import KafkaProducer\n",
    "from json import dumps\n",
    "\n",
    "# Kafka Configuration\n",
    "hostip = \"kafka\"\n",
    "\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        # Serializing the data, making sure all fields are strings\n",
    "        serialized_data = {key: (str(value) if key != 'ts' else value) for key, value in data.items()}\n",
    "        producer_instance.send(topic_name, value=serialized_data)\n",
    "        print(f'Message published successfully to {topic_name}. Data: {serialized_data}')\n",
    "    except Exception as ex:\n",
    "        print(f'Exception in publishing message: {str(ex)}')\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    try:\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=[f'{hostip}:9092'],\n",
    "            value_serializer=lambda x: dumps(x).encode('utf-8'),\n",
    "            api_version=(0, 10)\n",
    "        )\n",
    "        return producer\n",
    "    except Exception as ex:\n",
    "        print(f'Exception while connecting Kafka: {str(ex)}')\n",
    "        return None\n",
    "\n",
    "# Function to add Unix timestamps spread evenly over 5 seconds for browsing data.\n",
    "def add_timestamps(batch, start_time):\n",
    "    batch_size = len(batch)\n",
    "    rows_per_second = batch_size // 5\n",
    "    remaining_rows = batch_size % 5\n",
    "\n",
    "    for i in range(5):\n",
    "        ts = int(dt.datetime.timestamp(start_time) + i)\n",
    "        for j in range(rows_per_second):\n",
    "            index = i * rows_per_second + j\n",
    "            batch[index]['ts'] = ts\n",
    "\n",
    "    # Spread the remaining rows over the next 5 seconds\n",
    "    for k in range(remaining_rows):\n",
    "        index = 5 * rows_per_second + k\n",
    "        batch[index]['ts'] = int(dt.datetime.timestamp(start_time) + (k % 5))\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Streaming function for browsing behavior and transactions\n",
    "def stream_browsing_and_transactions(browsing_file, transactions_file, browsing_topic, transactions_topic):\n",
    "    try:\n",
    "        with open(browsing_file, mode='r') as browsing_f, open(transactions_file, mode='r') as transactions_f:\n",
    "            browsing_reader = csv.DictReader(browsing_f)\n",
    "            transactions_reader = csv.DictReader(transactions_f)\n",
    "            transaction_iterator = iter(transactions_reader)\n",
    "\n",
    "            producer = connect_kafka_producer()\n",
    "\n",
    "            while True:\n",
    "                # Random batch size between 500-1000\n",
    "                batch_size = random.randint(500, 1000)\n",
    "                browsing_batch = []\n",
    "\n",
    "                # Read the next random batch of browsing behavior lines\n",
    "                try:\n",
    "                    for _ in range(batch_size):\n",
    "                        browsing_batch.append(next(browsing_reader))\n",
    "                except StopIteration:\n",
    "                    print(f\"End of browsing file reached for {browsing_topic}.\")\n",
    "                    break\n",
    "\n",
    "                # Get the start time for the batch\n",
    "                start_time = dt.datetime.utcnow()\n",
    "\n",
    "                # Add timestamps to the browsing batch, spread across 5 seconds\n",
    "                browsing_batch = add_timestamps(browsing_batch, start_time)\n",
    "\n",
    "                # Get the min and max event_time from the browsing batch\n",
    "                browsing_event_times = [row['event_time'] for row in browsing_batch]\n",
    "                min_event_time = min(browsing_event_times)\n",
    "                max_event_time = max(browsing_event_times)\n",
    "\n",
    "                print(f\"Processing browsing batch with event time range: {min_event_time} to {max_event_time}\")\n",
    "\n",
    "                # Publish browsing data to Kafka\n",
    "                for record in browsing_batch:\n",
    "                    publish_message(producer, browsing_topic, record)\n",
    "\n",
    "                # Now process transactions within the same event time window\n",
    "                transaction_batch = []\n",
    "                try:\n",
    "                    while True:\n",
    "                        txn = next(transaction_iterator)\n",
    "                        # Filter transactions within the browsing time window\n",
    "                        if min_event_time <= txn['created_at'] <= max_event_time:\n",
    "                            transaction_batch.append(txn)\n",
    "                        elif txn['created_at'] > max_event_time:\n",
    "                            break\n",
    "                except StopIteration:\n",
    "                    print(f\"End of transaction file reached for {transactions_topic}.\")\n",
    "                    break\n",
    "\n",
    "                # Publish transactions without adding timestamps\n",
    "                if transaction_batch:\n",
    "                    for txn in transaction_batch:\n",
    "                        publish_message(producer, transactions_topic, txn)\n",
    "\n",
    "                # Wait time for 5 seconds before publishing the next batch\n",
    "                time.sleep(5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing files {browsing_file}, {transactions_file}: {e}\")\n",
    "    finally:\n",
    "        if producer:\n",
    "            producer.flush()\n",
    "            producer.close() \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Kafka topics for browsing and transactions\n",
    "    browsing_topic = 'browsing-behavior-topic'\n",
    "    transactions_topic = 'transactions-topic'\n",
    "\n",
    "    # Paths to the CSV files\n",
    "    browsing_file = \"new_browsing_behaviour.csv\"\n",
    "    transactions_file = \"new_transactions.csv\"\n",
    "\n",
    "    print('Publishing browsing behavior and transactions data to Kafka...')\n",
    "\n",
    "    # Stream browsing behavior and synchronize transactions by event time\n",
    "    stream_browsing_and_transactions(browsing_file, transactions_file, browsing_topic, transactions_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65151044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
